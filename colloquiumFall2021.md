---
layout: page
title: Fall 2021 Department Colloquium (preliminary schedule)
---

The talks will typically take place on Tuesdays at 4:00-5:00pm in Adel Room 164. Please contact <a href="mailto:nandor.sieben@nau.edu">Nandor Sieben</a> with questions about the colloquium.

<hr>

### Tuesday 8/31 at 4:00-4:20

**Speaker:** None

**Title:** Organizational meeting

**Abstract:** Please attend or email <a href="mailto:nandor.sieben@nau.edu">Nandor Sieben</a> before the meeting if you or your guest would like to give a talk this semester.

<hr>

### Tuesday 9/7 at 4:00-4:50

**Speaker:** Mitch Hamidi (Embry-Riddle Aeronautical University)

**Title:** Topological Dynamics: An Operator Algebraic Approach

**Abstract:** An operator algebra is an algebra of bounded linear operators acting on a Hilbert space that is closed in a certain norm topology. When that algebra is closed with respect to the adjoint operation (an abstract conjugate transpose), we call it a C*-algebra. The prototypical examples of \(C*\) -algebras include the ring of \( n \times n \) matrices over the complex numbers and the ring of complex-valued continuous functions on a compact Hausdorff space. The latter example gives an algebraic perspective for studying topological dynamics. In particular, one can build an operator algebra called a crossed product that encodes the dynamical information of a group of homeomorphisms acting on a topological space.

In the 1960s, W. Arveson determined that the action of a homeomorphism on a topological space is better encoded in a crossed product via the action of a semigroup on that space, rather than a group, which led to many important results in operator algebra theory. 

I will discuss how and why operator algebraists have been returning to crossed products in the context of groups acting on non-adjoint closed operator algebras, and I will discuss a recent partial solution to when dynamics are encoded fully in this crossed product context.


<hr>

### Tuesday 9/14 at 4:00-4:50

**Speaker:** Ye Chen

**Title:** Impacts of vaccination, Alpha and Delta variants on COVID-19 transmission dynamics in the 15 most populous metropolitan statistical areas in the United States.

**Abstract:**

<hr>

### Tuesday 9/21 at 4:00-4:50

**Speaker:** Jim Swift

**Title:** BLIS

**Abstract:**

<hr>

### Tuesday 9/28 at 4:00-4:50

**Speaker:** Rachel Neville

**Title:** 

**Abstract:**

<hr>


### Tuesday 10/5 at 4:00-4:50

**Speaker:** Dana Ernst

**Title:** 

**Abstract:**

<hr>

### Tuesday 10/12 at 4:00-4:50

**Speaker:** Angie Hodge-Zickerman

**Title:**

**Abstract:**

<hr>

### Tuesday 10/19 at 4:00-4:50

**Speaker:** Ryan Blackburn (student of Robert Buscaglia)

**Title:** Boruta Feature Selection for Forestry Predictions

**Abstract:**

<hr>

### Tuesday 10/26 at 4:00-4:50

**Speaker:** Brent Burch

**Title:**

**Abstract:**

<hr>

### Tuesday 11/2 or 11/16 at 4:00-4:50

**Speaker:** Gina Nabours

**Title:** LMC curriculum

**Abstract:**

<hr>

### Tuesday 11/9 4:00-4:50

**Speaker:** Toby D Hocking joint work with Jonathan Hillman

**Title:** Optimizing ROC Curves with a Sort-Based Surrogate Loss Function for Binary Classification and Changepoint Detection

**Abstract:** Receiver Operating Characteristic (ROC) curves are plots of true positive rate versus false positive rate which are useful for evaluating binary classification models, but difficult to use for learning since the Area Under the Curve (AUC) is non-convex. ROC curves can also be used in other problems that have false positive and true positive rates such as changepoint detection. We show that in this more general context, the ROC curve can have loops, points with highly sub-optimal error rates, and AUC greater than one. This observation motivates a new optimization objective: rather than maximizing the AUC, we would like a monotonic ROC curve with AUC=1 that avoids points with large values for Min(FP,FN). We propose a convex relaxation of this objective that results in a new surrogate loss function called the AUM, short for Area Under Min(FP, FN). Whereas previous loss functions are based on summing over all labeled examples or pairs, the AUM requires a sort and a sum over the sequence of points on the ROC curve. We show that AUM directional derivatives can be efficiently computed and used in a gradient descent learning algorithm. In our empirical study of supervised binary classification and changepoint detection problems, we show that our new AUM minimization learning algorithm results in improved AUC and comparable speed relative to previous baselines.

<hr>

### Tuesday 11/16 or 11/23 at 4:00-4:50

**Speaker:** Alyssa Whittemore

**Title:** 

**Abstract:**

<hr>

### Tuesday 11/16 or 11/23 at 4:00-4:50

**Speaker:** Mike Falk

**Title:** 

**Abstract:**

<hr>
