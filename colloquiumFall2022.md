---
layout: page
title: Fall 2022 Department Colloquium
---

The talks will typically take place on Tuesdays at 4:00-5:00pm in Adel Room 164. Please contact <a href="mailto:john.neuberger@nau.edu">John Neuberger</a> with questions about the colloquium.

Note that talks are listed in reverse chronological order.

<hr>

### Thursday 11/29 at 4:00-4:50

**Speaker:** Ryan Blackburn (NAU)

**Title:** Lidar-driven deep learning accurately classifies species in a southwestern mixed conifer forest

**Abstract:** Tree species classification using remotely sensed data is a challenge that has yet to be overcome. However, tree species classification is essential for forest monitoring and studying ecosystem dynamics especially over large spatial extents. Light detection and ranging (lidar) has shown promise in species classification but fails in complex forests or is limited to small extents. Recent advances of lidar processing and machine learning have provided an opportunity to take advantage of tree structural information. In this study, Deep learning was used to create a classification model for 7 species from lidar data within the Mogollon Rim Ranger District. This study demonstrates that ALS and UAV-LS acquisition types provide similar accuracies despite large differences in point densities and lays the groundwork for datasets to be modeled in different regions.

<hr>

### Thursday 11/17 at 4:00-4:50

**Speaker:** Levi Heath (University of Nebraska-Lincoln)

**Title:** Enumerative geometry in physics and quantum Serre duality for quasimaps

**Abstract:** An aim of enumerative geometry is to count intersection numbers. As a first example, given two distinct points in the complex projective plane, there is a unique line that intersects both points. Surprisingly, similar enumerative questions are related to a conjecture made by Edward Witten in theoretical physics. In 1992, the mathematician Maxim Kontsevich proved Witten's conjecture in his thesis and created a new area of mathematics called Gromov-Witten theory.

Let $X$ be a smooth variety and let $Z$ be a complete intersection in $X$ defined by a section of a vector bundle $E$ over $X$. Gromov-Witten and quasimap invariants of $X$ are integrals over Kontsevich's moduli space of stable maps to $X$ and Ciocan-Fontanine and Kim's moduli space of stable quasimaps respectively. Originally proposed by Givental, quantum Serre duality refers to a precise relationship between the Gromov-Witten invariants of $Z$ and those of the dual vector bundle $E^V$. In this talk, we motivate the study of Gromov-Witten and quasimap invariants, which have applications in theoretical physics, and present a quantum Serre duality statement for quasimap invariants. We describe how working with quasimaps allows us to obtain a comparison that is simpler, and that also holds in greater generality than previous quantum Serre duality results in Gromov-Witten theory. This is joint work with Mark Shoemaker.

<hr>

### Tuesday 11/8 at 4:00-4:50

**Speaker:** Bertrand Cambou (NAU)

**Title:** Introducing Lattice for Post Quantum Cryptography

**Abstract:** The asymmetrical cryptographic algorithms such as RSA, and Elliptic Curve Cryptography, are we think vulnerable to powerful quantum computers. The program lead by NIST on post quantum cryptography (PQC) is currently considering algorithms based on lattices as strong candidates to replace legacy codes. In this talk, we are going to briefly review some of the mathematical elements describing lattices, and the arithmetic of truncated polynomials. The scheme called "learning with error (LWE)" will be presented, as well as its use in cryptography Finally, we will discuss the implementation of "Dilithium" and "Kyber", the two LWE-based PQC algorithms under consideration by NIST for standardization, and some of the research work conducted at NAU in the field.

<hr>

### Tuesday 11/1 at 4:00-4:50

**Speaker:** Bianca Luedeker (NAU)

**Title:** Compositional Data Analysis and the Nested Dirichlet Distribution

**Abstract:** Compositional data is a special type of multivariate data where each component of the data vector is sandwiched between 0 and 1 and the sum of the components must be 1. For example, the proportion of time that each of 7 mice spend in one of four quadrants of a circular water maze is between 0 and 1,
and the total proportion of time spent in the maze is 1. In this case, the proportion of time spent in each quadrant of the maze is a "component". If there are two sets of mice, one set of normal mice and one set of cognitively impaired mice, the experiment has a two-sample design. Such data is frequently analyzed incorrectly by comparing the two samples via a t-test (or ANOVA for multiple samples) on one component of the vector at a time.

In this dissertation, this problem is corrected by analyzing compositional datasets using nested Dirichlet distributions, generalized versions of Dirichlet distributions that allow for positive correlations among components. Specifically, we extend a previous result of two-sample comparisons using Dirichlet distributions and nested Dirichlet distributions to multi-sample comparisons. The performance of the new test in terms of type I error rates and power is established using simulation studies. In addition, to use a nested model, an appropriate tree which describes the relationship between components must first be found. An existing data driven tree finding algorithm is improved upon by including an extra step that prunes unnecessary nodes using confidence intervals for the differences between parameters at each level of the tree. The tree finding algorithm and multi-sample test are demonstrated on two datasets. The first dataset measures the proportion of home runs, triples, doubles, singles, outs, and other events for batters on professional baseball teams from three age groups. The second dataset compares the composition of job types in the United States by region.

<hr>

### Tuesday 10/25 at 4:00-4:50

**Speaker:** Ye Chen (NAU)

**Title:** Compartmental epidemiological models and Bayesian inference for parameter estimation

**Abstract:** Epidemiological models play a vital role in understanding the spread and severity of a pandemic of infectious disease, such as the COVID-19. The mathematical modeling of infectious diseases in the form of compartmental models are often employed in studying the probable outbreak growth. Such models heavily rely on a good estimation of the epidemiological parameters for simulating the outbreak trajectory. Bayesian estimation is a commonly used technique in parameters inference of compartmental models. However, the high computational cost of inferring from a complicated and often intractable ‘true posterior distribution’ has always been an obstacle in the Bayesian framework. In this talk, I will give an overview of some epidemiological models and a new Bayesian estimation approach I have been working on.

<hr>

### Tuesday 10/18 at 4:00-4:50

**Speaker:** Natali Hritonenko (Prairie View A&M, Texas)

**Title:** To Mitigate or Adapt to Climate Change: Modeling approach

**Abstract:** The presentation will start with a brief introduction of ongoing and prospective research directions. The talk will focus on mathematical modelling and analysis of two related contemporary issues (a) modernization of industrial equipment under fast technological development and (b) environment adaptation and pollution mitigation as two main strategies to deal with climate change and industrial contamination.

The considered deterministic ODE model that portrays complex interrelations of production, pollution, and the environmental protection is a result of intensive collaborative efforts of mathematicians, economists, and environmentalists.
Volterra integral equations of the second kind are used to model rational capital replacement under scientific and technological innovations, government quotas, and environmental constraints.

Different extensions and modifications of the models as well as developed methods of their investigation and applied interpretation of obtained results will be presented. A short survey of other quantitative and qualitative modelling approaches to solve these problems will be provided.

<hr>

### Tuesday 10/11 at 4:00-4:50

**Speaker:** Jaechoul Lee (NAU)

**Title:** Lag time between state-level policy interventions and changepoints in U.S. COVID-19 outcomes

**Abstract:** Public health policy implemented at the state level has been important in managing the spread of
COVID-19. State governments have had to balance the need to reduce the spread of COVID-19 with the public's desire to return to normal life. As a result, there are both restrictive policies and reopening policies that influence social behavior and consequentially the spread of COVID-19. Here, we describe the relationship between changes in the trajectory of COVID-19 cases and deaths and policy implementations. For this, we first detect changepoints in the COVID-19 outcomes using a data-driven search algorithm and then relate these changepoints to implemented policies. Particularly, we show that there is a change in COVID-19 outcomes approximately 10–14 days after state-level policy implementation. This work can help health officials understand the time it will take for state-level policies to have an impact on the trajectory of a highly infectious illness like COVID-19. Knowing that there is a significant lag time between policy implementation and its effect on the spread of disease can help officials be more proactive in responding to health crises. Limitations and further research opportunities will be discussed at the end of the talk.

<hr>

### Tuesday 10/3 at 4:00-4:50

**Speaker:** Angie Hodge (NAU)

**Title:** Math Pathways: Arizona and Beyond

**Abstract:** In August 2022, Arizona was selected to be a part of the Dana Center’s Launch Years Initiative. Come find out what all of this means to Arizona and specifically to the NAU Department of Mathematics and Statistics. In this talk, I will share the work I’ve been doing with the Arizona Pathways statewide team (including monthly virtual meetings and a meeting in DC with the Conference Board of Mathematical Sciences). I will also gather feedback from attendees on their thoughts about the initiative.

<hr>

### Tuesday 9/27 at 4:00-4:50

**Speaker:** John Neuberger (NAU)

**Title:** Splines, Direction Fields, and ODE Solvers

**Abstract:** We will start with an introduction to the famous cubic spline interpolator from MAT 362 Intro to Numerical Analysis, which we implement as a single linear system defined with block matrices. We will see how to modify the linear system to approximate solutions to linear ODE. For nonlinear ODE we obtain a nonlinear system, which can be solved by Newton’s method. The talk concludes with some ideas about various algorithms derived from this way of thinking, along with some open questions and ongoing research topics.

<hr>

### Tuesday 9/13 at 4:00-4:50

**Speaker:** Michael Falk (NAU)

**Title:** Twin groups and doodles

**Abstract:** I will give an introduction to twin groups as an excuse to talk about classical configuration spaces, configuration spaces on graphs, hyperplane arrangements, and Coxeter groups. Twin groups were discovered or invented by Mikhael Khovanov in the mid-1990's, before he famously categorified the Jones polynomial. The origins lie in the $K(\pi,1)$ problem for hyperplane arrangements, and the groups themselves are right-angled Coxeter groups related to doodles on the two-dimensional sphere and Grothendieck's dessins d'enfant, which aspects I will explain to the extent I am able. The title of the talk is from Khovanov's paper.

<hr>

### Tuesday 9/6 at 4:00-4:50

**Speaker:** Jim Swift (NAU)

**Title:** Solving $2\times 2$ matrix Initial Value Problems without computing eigenvectors

**Abstract:** We present a simple algorithm for the solution to the ordinary differential equation $dx/dt = Ax$ with the initial condition $x(0) = x_0$, when $A$ is a $2\times 2$ real matrix. Solving this initial value problem is a topic in our lower division differential equations class, MAT 239 Ordinary Differential Equations. My solution does not require a computation of the eigenvectors of $A$, and is many times faster than the standard techniques found in the textbooks. But is the technique too easy, too cookbook, even for MAT 239? Should we teach this method to our students? I will invite a discussion.
This talk is in person with no zoom option. Knowledge of MAT 239 will be helpful, but not required, to understand the talk.

<hr>
